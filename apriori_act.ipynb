{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "df = pd.DataFrame(data=iris.data, columns=iris.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discretize numerical features into three bins\n",
    "for feature in df.columns:\n",
    "    df[feature] = pd.cut(df[feature], bins=3, labels=['low', 'medium', 'high'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "UFuncTypeError",
     "evalue": "ufunc 'multiply' did not contain a loop with signature matching types (dtype('<U3'), dtype('float64')) -> None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUFuncTypeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Discretize numerical features into three bins\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[1;32m----> 3\u001b[0m     df[feature] \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcut\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlow\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmedium\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhigh\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\johnl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\reshape\\tile.py:246\u001b[0m, in \u001b[0;36mcut\u001b[1;34m(x, bins, right, labels, retbins, precision, include_lowest, duplicates, ordered)\u001b[0m\n\u001b[0;32m    243\u001b[0m x_idx, _ \u001b[38;5;241m=\u001b[39m _coerce_to_type(x_idx)\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39miterable(bins):\n\u001b[1;32m--> 246\u001b[0m     bins \u001b[38;5;241m=\u001b[39m \u001b[43m_nbins_to_bins\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbins\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(bins, IntervalIndex):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m bins\u001b[38;5;241m.\u001b[39mis_overlapping:\n",
      "File \u001b[1;32mc:\\Users\\johnl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\reshape\\tile.py:403\u001b[0m, in \u001b[0;36m_nbins_to_bins\u001b[1;34m(x_idx, nbins, right)\u001b[0m\n\u001b[0;32m    399\u001b[0m     bins \u001b[38;5;241m=\u001b[39m x_idx\u001b[38;5;241m.\u001b[39m_values\u001b[38;5;241m.\u001b[39m_generate_range(  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[0;32m    400\u001b[0m         start\u001b[38;5;241m=\u001b[39mmn, end\u001b[38;5;241m=\u001b[39mmx, periods\u001b[38;5;241m=\u001b[39mnbins \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, unit\u001b[38;5;241m=\u001b[39munit\n\u001b[0;32m    401\u001b[0m     )\n\u001b[0;32m    402\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 403\u001b[0m     bins \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinspace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbins\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    404\u001b[0m adj \u001b[38;5;241m=\u001b[39m (mx \u001b[38;5;241m-\u001b[39m mn) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.001\u001b[39m  \u001b[38;5;66;03m# 0.1% of the range\u001b[39;00m\n\u001b[0;32m    405\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m right:\n",
      "File \u001b[1;32mc:\\Users\\johnl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\core\\function_base.py:129\u001b[0m, in \u001b[0;36mlinspace\u001b[1;34m(start, stop, num, endpoint, retstep, dtype, axis)\u001b[0m\n\u001b[0;32m    125\u001b[0m div \u001b[38;5;241m=\u001b[39m (num \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m endpoint \u001b[38;5;28;01melse\u001b[39;00m num\n\u001b[0;32m    127\u001b[0m \u001b[38;5;66;03m# Convert float/complex array scalars to float, gh-3504\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;66;03m# and make sure one can use variables that have an __array_interface__, gh-6634\u001b[39;00m\n\u001b[1;32m--> 129\u001b[0m start \u001b[38;5;241m=\u001b[39m \u001b[43masanyarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\n\u001b[0;32m    130\u001b[0m stop  \u001b[38;5;241m=\u001b[39m asanyarray(stop)  \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[0;32m    132\u001b[0m dt \u001b[38;5;241m=\u001b[39m result_type(start, stop, \u001b[38;5;28mfloat\u001b[39m(num))\n",
      "\u001b[1;31mUFuncTypeError\u001b[0m: ufunc 'multiply' did not contain a loop with signature matching types (dtype('<U3'), dtype('float64')) -> None"
     ]
    }
   ],
   "source": [
    "# Discretize numerical features into three bins\n",
    "for feature in df.columns:\n",
    "    df[feature] = pd.cut(df[feature], bins=3, labels=['low', 'medium', 'high'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to one-hot encoded format\n",
    "oht = pd.get_dummies(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   antecedents consequents  antecedent support  consequent support  support  \\\n",
      "0          (B)         (A)                 1.0                 1.0      1.0   \n",
      "1          (A)         (B)                 1.0                 1.0      1.0   \n",
      "2          (C)         (A)                 0.6                 1.0      0.6   \n",
      "3          (A)         (C)                 1.0                 0.6      0.6   \n",
      "4          (B)         (C)                 1.0                 0.6      0.6   \n",
      "5          (C)         (B)                 0.6                 1.0      0.6   \n",
      "6       (B, C)         (A)                 0.6                 1.0      0.6   \n",
      "7       (B, A)         (C)                 1.0                 0.6      0.6   \n",
      "8       (C, A)         (B)                 0.6                 1.0      0.6   \n",
      "9          (B)      (C, A)                 1.0                 0.6      0.6   \n",
      "10         (C)      (B, A)                 0.6                 1.0      0.6   \n",
      "11         (A)      (B, C)                 1.0                 0.6      0.6   \n",
      "\n",
      "    confidence  lift  leverage  conviction  zhangs_metric  \n",
      "0          1.0   1.0       0.0         inf            0.0  \n",
      "1          1.0   1.0       0.0         inf            0.0  \n",
      "2          1.0   1.0       0.0         inf            0.0  \n",
      "3          0.6   1.0       0.0         1.0            0.0  \n",
      "4          0.6   1.0       0.0         1.0            0.0  \n",
      "5          1.0   1.0       0.0         inf            0.0  \n",
      "6          1.0   1.0       0.0         inf            0.0  \n",
      "7          0.6   1.0       0.0         1.0            0.0  \n",
      "8          1.0   1.0       0.0         inf            0.0  \n",
      "9          0.6   1.0       0.0         1.0            0.0  \n",
      "10         1.0   1.0       0.0         inf            0.0  \n",
      "11         0.6   1.0       0.0         1.0            0.0  \n"
     ]
    }
   ],
   "source": [
    "# Find frequent itemsets with a minimum support of 0.1\n",
    "frequent_itemsets = apriori(oht, min_support=0.1, use_colnames=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate association rules with a minimum lift of 1.0\n",
    "rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rules)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         antecedents  \\\n",
      "0                            (sepal length (cm)_low)   \n",
      "1                            (sepal width (cm)_high)   \n",
      "2                            (petal length (cm)_low)   \n",
      "3                            (sepal length (cm)_low)   \n",
      "4                             (petal width (cm)_low)   \n",
      "..                                               ...   \n",
      "177  (petal length (cm)_high, petal width (cm)_high)   \n",
      "178                        (sepal width (cm)_medium)   \n",
      "179                         (sepal length (cm)_high)   \n",
      "180                         (petal length (cm)_high)   \n",
      "181                          (petal width (cm)_high)   \n",
      "\n",
      "                                           consequents  antecedent support  \\\n",
      "0                              (sepal width (cm)_high)            0.346667   \n",
      "1                              (sepal length (cm)_low)            0.286667   \n",
      "2                              (sepal length (cm)_low)            0.333333   \n",
      "3                              (petal length (cm)_low)            0.346667   \n",
      "4                              (sepal length (cm)_low)            0.333333   \n",
      "..                                                 ...                 ...   \n",
      "177  (sepal width (cm)_medium, sepal length (cm)_high)            0.273333   \n",
      "178  (sepal length (cm)_high, petal length (cm)_hig...            0.333333   \n",
      "179  (sepal width (cm)_medium, petal length (cm)_hi...            0.280000   \n",
      "180  (sepal width (cm)_medium, sepal length (cm)_hi...            0.306667   \n",
      "181  (sepal width (cm)_medium, sepal length (cm)_hi...            0.320000   \n",
      "\n",
      "     consequent support   support  confidence      lift  leverage  conviction  \\\n",
      "0              0.286667  0.186667    0.538462  1.878354  0.087289    1.545556   \n",
      "1              0.346667  0.186667    0.651163  1.878354  0.087289    1.872889   \n",
      "2              0.346667  0.300000    0.900000  2.596154  0.184444    6.533333   \n",
      "3              0.333333  0.300000    0.865385  2.596154  0.184444    4.952381   \n",
      "4              0.346667  0.300000    0.900000  2.596154  0.184444    6.533333   \n",
      "..                  ...       ...         ...       ...       ...         ...   \n",
      "177            0.166667  0.120000    0.439024  2.634146  0.074444    1.485507   \n",
      "178            0.206667  0.120000    0.360000  1.741935  0.051111    1.239583   \n",
      "179            0.126667  0.120000    0.428571  3.383459  0.084533    1.528333   \n",
      "180            0.120000  0.120000    0.391304  3.260870  0.083200    1.445714   \n",
      "181            0.126667  0.120000    0.375000  2.960526  0.079467    1.397333   \n",
      "\n",
      "     zhangs_metric  \n",
      "0         0.715743  \n",
      "1         0.655541  \n",
      "2         0.922222  \n",
      "3         0.941043  \n",
      "4         0.922222  \n",
      "..             ...  \n",
      "177       0.853721  \n",
      "178       0.638889  \n",
      "179       0.978395  \n",
      "180       1.000000  \n",
      "181       0.973856  \n",
      "\n",
      "[182 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "\n",
    "# Convert data to categorical format\n",
    "for feature in df.columns:\n",
    "    df[feature] = pd.qcut(df[feature], q=3, labels=['low', 'medium', 'high'])\n",
    "\n",
    "# Convert data to one-hot encoded format\n",
    "oht = pd.get_dummies(df)\n",
    "\n",
    "# Find frequent itemsets with a minimum support of 0.1\n",
    "frequent_itemsets = apriori(oht, min_support=0.1, use_colnames=True)\n",
    "\n",
    "# Generate association rules with a minimum lift of 1.0\n",
    "rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1.0)\n",
    "\n",
    "# Display results\n",
    "print(rules)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
